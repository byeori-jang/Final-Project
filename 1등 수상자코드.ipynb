{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237c2e4c-073c-4456-a7dc-0cef03053864",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309209d-98bf-4343-af2d-5d6bc99c31f8",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b8aae7-be6e-4b5b-b24c-6dd93c54e255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\te\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcatboost\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier, Pool\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, StratifiedKFold\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import optuna\n",
    "import os\n",
    "import json\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdfac02-bfd3-4443-a8b9-925bbec5658f",
   "metadata": {},
   "source": [
    "## Data Organize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6472c0ab-403e-4df6-bc06-b37595d2766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [\"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "categories = [\"íšŒì›ì •ë³´\", \"ì‹ ìš©ì •ë³´\", \"ìŠ¹ì¸ë§¤ì¶œì •ë³´\", \"ì²­êµ¬ì…ê¸ˆì •ë³´\", \"ì”ì•¡ì •ë³´\", \"ì±„ë„ì •ë³´\", \"ë§ˆì¼€íŒ…ì •ë³´\", \"ì„±ê³¼ì •ë³´\"]\n",
    "data_types = [\"train\", \"test\"]\n",
    "\n",
    "def merge_monthly_data(data_type, category):\n",
    "    merged_list = []\n",
    "    for month in months:\n",
    "        file_name = f\"./{data_type}/{category}/2018{month}_{data_type}_{category}.parquet\"\n",
    "        try:\n",
    "            df = pd.read_parquet(file_name, engine=\"pyarrow\")\n",
    "            merged_list.append(df)\n",
    "            print(f\"âœ… {file_name} ë³€í™˜ ì™„ë£Œ\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_name}\")\n",
    "    if merged_list:\n",
    "        merged_df = pd.concat(merged_list, ignore_index=True)\n",
    "        output_file = f\"./{data_type}/{category}/{data_type}_{category}.csv\"\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"âœ… {output_file} ì €ì¥ ì™„ë£Œ (Shape: {merged_df.shape})\")\n",
    "    else:\n",
    "        print(f\"âŒ {data_type}_{category} ë°ì´í„° ì—†ìŒ\")\n",
    "\n",
    "for data_type in data_types:\n",
    "    for category in categories:\n",
    "        merge_monthly_data(data_type, category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e38c0-1cf8-45a7-a154-815fd07a0fe0",
   "metadata": {},
   "source": [
    "## Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bcf7d4-6826-49cf-8486-0a77b4fe8ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "    \"train_íšŒì›ì •ë³´.csv\",\n",
    "    \"train_ì‹ ìš©ì •ë³´.csv\",\n",
    "    \"train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.csv\",\n",
    "    \"train_ì²­êµ¬ì…ê¸ˆì •ë³´.csv\",\n",
    "    \"train_ì”ì•¡ì •ë³´.csv\",\n",
    "    \"train_ì±„ë„ì •ë³´.csv\",\n",
    "    \"train_ë§ˆì¼€íŒ…ì •ë³´.csv\",\n",
    "    \"train_ì„±ê³¼ì •ë³´.csv\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(f\"./train/{categories[0]}/{file_names[0]}\")\n",
    "for idx, file in enumerate(file_names[1:], start=2):\n",
    "    print(f\"\\nğŸ”¹ ë³‘í•© ì¤‘: {file} ({idx}/{len(file_names)})\")\n",
    "    temp_df = pd.read_csv(f\"./train/{categories[idx-1]}/{file}\")\n",
    "    df = df.merge(temp_df, how=\"left\", on=[\"ID\", \"ê¸°ì¤€ë…„ì›”\"])\n",
    "    print(f\"âœ… ë³‘í•© í›„ í¬ê¸°: {df.shape}\")\n",
    "\n",
    "output_file = \"./train/base_train.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ… ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}\")\n",
    "print(f\"ğŸ§¾ ìµœì¢… ë°ì´í„° í¬ê¸°: {df.shape[0]}í–‰, {df.shape[1]}ì—´\")\n",
    "file_names = [\n",
    "    \"test_íšŒì›ì •ë³´.csv\",\n",
    "    \"test_ì‹ ìš©ì •ë³´.csv\",\n",
    "    \"test_ìŠ¹ì¸ë§¤ì¶œì •ë³´.csv\",\n",
    "    \"test_ì²­êµ¬ì…ê¸ˆì •ë³´.csv\",\n",
    "    \"test_ì”ì•¡ì •ë³´.csv\",\n",
    "    \"test_ì±„ë„ì •ë³´.csv\",\n",
    "    \"test_ë§ˆì¼€íŒ…ì •ë³´.csv\",\n",
    "    \"test_ì„±ê³¼ì •ë³´.csv\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(f\"./test/{categories[0]}/{file_names[0]}\")\n",
    "for idx, file in enumerate(file_names[1:], start=2):\n",
    "    print(f\"\\nğŸ”¹ ë³‘í•© ì¤‘: {file} ({idx}/{len(file_names)})\")\n",
    "    temp_df = pd.read_csv(f\"./test/{categories[idx-1]}/{file}\")\n",
    "    df = df.merge(temp_df, how=\"left\", on=[\"ID\", \"ê¸°ì¤€ë…„ì›”\"])\n",
    "    print(f\"âœ… ë³‘í•© í›„ í¬ê¸°: {df.shape}\")\n",
    "\n",
    "output_file = \"./test/base_test.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ… ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}\")\n",
    "print(f\"ğŸ§¾ ìµœì¢… ë°ì´í„° í¬ê¸°: {df.shape[0]}í–‰, {df.shape[1]}ì—´\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd4ecf8-6f5b-40b1-a03d-060be5501ce3",
   "metadata": {},
   "source": [
    "## Data Preprocessing - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a20f0-b929-4e9d-a01a-dc8a73204473",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "    \"train_íšŒì›ì •ë³´.csv\",\n",
    "    \"train_ì‹ ìš©ì •ë³´.csv\",\n",
    "    \"train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.csv\",\n",
    "    \"train_ì²­êµ¬ì…ê¸ˆì •ë³´.csv\",\n",
    "    \"train_ì”ì•¡ì •ë³´.csv\",\n",
    "    \"train_ì±„ë„ì •ë³´.csv\",\n",
    "    \"train_ë§ˆì¼€íŒ…ì •ë³´.csv\",\n",
    "    \"train_ì„±ê³¼ì •ë³´.csv\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(f\"./train/{categories[0]}/{file_names[0]}\")\n",
    "original_shape = df.shape\n",
    "\n",
    "for idx, file in enumerate(file_names[1:], start=2):\n",
    "    print(f\"\\nğŸ”¹ ë³‘í•© ì§„í–‰ ì¤‘: {file} (íŒŒì¼ {idx} / {len(file_names)})\")\n",
    "    temp_df = pd.read_csv(f\"./train/{categories[idx-1]}/{file}\")\n",
    "    df = df.merge(temp_df, how=\"left\", on=[\"ID\", \"ê¸°ì¤€ë…„ì›”\"])\n",
    "    print(f\"âœ… ë³‘í•© í›„ ë°ì´í„° í¬ê¸°: {df.shape[0]}í–‰, {df.shape[1]}ì—´\")\n",
    "\n",
    "    constant_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
    "    if constant_cols:\n",
    "        print(f\"ğŸ“Œ ì œê±°ëœ ëª¨ë“  ê°’ì´ ë™ì¼í•œ ì¹¼ëŸ¼: {constant_cols}\")\n",
    "        df = df.drop(columns=constant_cols)\n",
    "    else:\n",
    "        print(\"ğŸ“Œ ëª¨ë“  ê°’ì´ ë™ì¼í•œ ì¹¼ëŸ¼ ì—†ìŒ\")\n",
    "\n",
    "    col_groups = {}\n",
    "    for col in df.columns:\n",
    "        for key in col_groups:\n",
    "            if df[col].equals(df[key]):\n",
    "                col_groups[key].append(col)\n",
    "                break\n",
    "        else:\n",
    "            col_groups[col] = [col]\n",
    "\n",
    "    duplicate_cols = [col for group in col_groups.values() for col in group[1:]]\n",
    "    if duplicate_cols:\n",
    "        print(f\"ğŸ“Œ ì œê±°ëœ ì¤‘ë³µ ì¹¼ëŸ¼: {duplicate_cols}\")\n",
    "        df = df.drop(columns=duplicate_cols)\n",
    "    else:\n",
    "        print(\"ğŸ“Œ ì¤‘ë³µ ì¹¼ëŸ¼ ì—†ìŒ\")\n",
    "\n",
    "    if 'ID' in df.columns and df.columns.str.contains('ID').sum() > 1:\n",
    "        df = df.loc[:, ~df.columns.str.contains('ID', case=False)].join(df[['ID']])\n",
    "\n",
    "    if 'ê¸°ì¤€ë…„ì›”' in df.columns and df.columns.str.contains('ê¸°ì¤€ë…„ì›”').sum() > 1:\n",
    "        df = df.loc[:, ~df.columns.str.contains('ê¸°ì¤€ë…„ì›”', case=False)].join(df[['ê¸°ì¤€ë…„ì›”']])\n",
    "\n",
    "    print(f\"ğŸ”¹ {file} ì²˜ë¦¬ ì™„ë£Œ. í˜„ì¬ ë°ì´í„° í¬ê¸°: {df.shape[0]}í–‰, {df.shape[1]}ì—´\")\n",
    "\n",
    "new_shape = df.shape\n",
    "output_file = \"./train/base_clean_train.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ì›ë˜ ë°ì´í„° í¬ê¸°: {original_shape[0]}í–‰, {original_shape[1]}ì—´\")\n",
    "print(f\"âœ… ë³‘í•© í›„ ìµœì¢… ë°ì´í„° í¬ê¸°: {new_shape[0]}í–‰, {new_shape[1]}ì—´\")\n",
    "print(f\"\\nâœ… ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}\")\n",
    "test_file_names = [\n",
    "    \"test_íšŒì›ì •ë³´.csv\",\n",
    "    \"test_ì‹ ìš©ì •ë³´.csv\",\n",
    "    \"test_ìŠ¹ì¸ë§¤ì¶œì •ë³´.csv\",\n",
    "    \"test_ì²­êµ¬ì…ê¸ˆì •ë³´.csv\",\n",
    "    \"test_ì”ì•¡ì •ë³´.csv\",\n",
    "    \"test_ì±„ë„ì •ë³´.csv\",\n",
    "    \"test_ë§ˆì¼€íŒ…ì •ë³´.csv\",\n",
    "    \"test_ì„±ê³¼ì •ë³´.csv\"\n",
    "]\n",
    "\n",
    "test_df = pd.read_csv(f\"./test/{categories[0]}/{test_file_names[0]}\")\n",
    "test_original_shape = test_df.shape\n",
    "\n",
    "for idx, file in enumerate(test_file_names[1:], start=2):\n",
    "    print(f\"\\nğŸ”¹ ë³‘í•© ì§„í–‰ ì¤‘: {file} (íŒŒì¼ {idx} / {len(test_file_names)})\")\n",
    "    temp_df = pd.read_csv(f\"./test/{categories[idx-1]}/{file}\")\n",
    "    test_df = test_df.merge(temp_df, how=\"left\", on=[\"ID\", \"ê¸°ì¤€ë…„ì›”\"])\n",
    "    print(f\"âœ… ë³‘í•© í›„ ë°ì´í„° í¬ê¸°: {test_df.shape[0]}í–‰, {test_df.shape[1]}ì—´\")\n",
    "\n",
    "train_df = pd.read_csv(\"./train/base_clean_train.csv\", nrows=1)\n",
    "train_columns = train_df.columns\n",
    "test_columns_to_keep = [col for col in test_df.columns if col in train_columns]\n",
    "\n",
    "test_df = test_df[test_columns_to_keep]\n",
    "test_final_shape = test_df.shape\n",
    "test_output_file = \"./test/base_clean_test.csv\"\n",
    "test_df.to_csv(test_output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ì›ë˜ test ë°ì´í„° í¬ê¸°: {test_original_shape[0]}í–‰, {test_original_shape[1]}ì—´\")\n",
    "print(f\"âœ… ë³‘í•© í›„ ìµœì¢… test ë°ì´í„° í¬ê¸°: {test_final_shape[0]}í–‰, {test_final_shape[1]}ì—´\")\n",
    "print(f\"\\nâœ… ìµœì¢… test ë°ì´í„° ì €ì¥ ì™„ë£Œ: {test_output_file}\")\n",
    "\n",
    "train_col_set = set(train_columns)\n",
    "test_col_set = set(test_df.columns)\n",
    "if train_col_set == test_col_set:\n",
    "    print(\"\\nâœ… trainê³¼ testì˜ ì»¬ëŸ¼ì´ ì™„ì „íˆ ì¼ì¹˜í•©ë‹ˆë‹¤!\")\n",
    "else:\n",
    "    train_only_cols = train_col_set - test_col_set\n",
    "    test_only_cols = test_col_set - train_col_set\n",
    "    print(f\"\\nâš ï¸ trainê³¼ testì˜ ì»¬ëŸ¼ì´ ë‹¤ë¦…ë‹ˆë‹¤!\")\n",
    "    print(f\"ğŸ”¹ trainì—ë§Œ ìˆëŠ” ì»¬ëŸ¼ ({len(train_only_cols)}ê°œ): {train_only_cols}\")\n",
    "    print(f\"ğŸ”¹ testì—ë§Œ ìˆëŠ” ì»¬ëŸ¼ ({len(test_only_cols)}ê°œ): {test_only_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f07e1-f774-427d-a2bc-8cbd03581479",
   "metadata": {},
   "source": [
    "## Modeling - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa90b03-b4e1-4aca-8315-fe3936ba87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train/base_clean_train.csv')\n",
    "test = pd.read_csv('./test/base_clean_test.csv')\n",
    "\n",
    "ab_ids = train[train['Segment'].isin(['A', 'B'])]['ID'].unique()\n",
    "train = train[~train['ID'].isin(ab_ids)].copy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train['Segment'] = label_encoder.fit_transform(train['Segment'])\n",
    "\n",
    "X = train.drop(columns=['Segment', 'ID'])\n",
    "y = train['Segment']\n",
    "X_test = test.drop(columns=['ID'])\n",
    "\n",
    "cat_features = [col for col in X.columns if X[col].dtype == 'object']\n",
    "for col in cat_features:\n",
    "    X[col] = X[col].astype(str)\n",
    "    X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "best_params = {\n",
    "    \"bootstrap_type\": \"Bayesian\",\n",
    "    \"learning_rate\": 0.2997682904093563,\n",
    "    \"l2_leaf_reg\": 9.214022161348987,\n",
    "    \"random_strength\": 7.342192789415524,\n",
    "    \"bagging_temperature\": 0.11417356499443036,\n",
    "    \"border_count\": 251,\n",
    "    \"iterations\": 1500,\n",
    "    \"loss_function\": \"MultiClass\",\n",
    "    \"eval_metric\": \"TotalF1\",\n",
    "    \"task_type\": \"GPU\",\n",
    "    \"verbose\": 100,\n",
    "    \"random_seed\": 42,\n",
    "    \"depth\": 8,\n",
    "    \"class_weights\": [2, 1, 1]\n",
    "}\n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "all_test_probs = np.zeros((X_test.shape[0], n_classes))\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"ğŸš€ Fold {fold+1} training...\")\n",
    "    X_train_fold, y_train_fold = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_valid_fold, y_valid_fold = X.iloc[valid_idx], y.iloc[valid_idx]\n",
    "    model = CatBoostClassifier(**best_params)\n",
    "    model.fit(X_train_fold, y_train_fold, cat_features=cat_features)\n",
    "    fold_probs = model.predict_proba(X_test)\n",
    "    all_test_probs += fold_probs\n",
    "\n",
    "avg_test_probs = all_test_probs / kf.get_n_splits()\n",
    "prob_df = pd.DataFrame(avg_test_probs, columns=range(n_classes))\n",
    "prob_df['ID'] = test['ID'].values\n",
    "\n",
    "mean_probs = prob_df.groupby('ID').mean().reset_index()\n",
    "mean_probs['Segment'] = mean_probs.drop(columns='ID').values.argmax(axis=1)\n",
    "segment_mapping = {0: 'C', 1: 'D', 2: 'E'}\n",
    "mean_probs['Segment'] = mean_probs['Segment'].map(segment_mapping)\n",
    "\n",
    "submission = pd.DataFrame({'ID': mean_probs['ID'], 'Segment': mean_probs['Segment']})\n",
    "submission.to_csv('./test/base_catboost_kfold.csv', index=False)\n",
    "print(\"âœ… CatBoost + 10-Fold CV ì˜ˆì¸¡ ì™„ë£Œ ë° ì €ì¥ ğŸ¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be081de-ead5-4c83-b88c-4887983701f1",
   "metadata": {},
   "source": [
    "## Data Preprocessing - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414a7fb-adae-40b7-9993-47cbeb9f8066",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train/base_clean_train.csv')\n",
    "test = pd.read_csv('./test/base_clean_test.csv')\n",
    "\n",
    "train_A = train[train['Segment'] == 'A']\n",
    "cols_to_check = [col for col in train.columns if col not in ['ID', 'Segment']]\n",
    "\n",
    "def is_fixed_column(df, col):\n",
    "    return df[col].nunique() == 1\n",
    "\n",
    "fixed_columns_A = {col: train_A[col].iloc[0] for col in cols_to_check if is_fixed_column(train_A, col)}\n",
    "fixed_cols = list(fixed_columns_A.keys())\n",
    "print(f\"ğŸ“¦ ê³ ì •ëœ ì¹¼ëŸ¼ {len(fixed_cols)}ê°œ ì œê±°í•  ì˜ˆì •ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "matching_ids_train = train.copy()\n",
    "for col, value in fixed_columns_A.items():\n",
    "    matching_ids_train = matching_ids_train[matching_ids_train[col] == value]\n",
    "matching_ids_train_list = matching_ids_train.groupby('ID').filter(lambda x: len(x) == 6)['ID'].unique()\n",
    "\n",
    "matching_ids_test = test.copy()\n",
    "for col, value in fixed_columns_A.items():\n",
    "    matching_ids_test = matching_ids_test[matching_ids_test[col] == value]\n",
    "matching_ids_test_list = matching_ids_test.groupby('ID').filter(lambda x: len(x) == 6)['ID'].unique()\n",
    "\n",
    "train_filtered = train[train['ID'].isin(matching_ids_train_list)].drop(columns=fixed_cols)\n",
    "test_filtered = test[test['ID'].isin(matching_ids_test_list)].drop(columns=fixed_cols)\n",
    "\n",
    "print(f\"ğŸš€ ìµœì¢… train ë°ì´í„° shape: {train_filtered.shape}\")\n",
    "print(f\"ğŸš€ ìµœì¢… test ë°ì´í„° shape: {test_filtered.shape}\")\n",
    "train_filtered.to_csv('./train/train_vips_A.csv', index=False)\n",
    "test_filtered.to_csv('./test/test_vips_A.csv', index=False)\n",
    "train = pd.read_csv('./train/base_clean_train.csv')\n",
    "test = pd.read_csv('./test/base_clean_test.csv')\n",
    "\n",
    "train_B = train[train['Segment'] == 'B']\n",
    "cols_to_check = [col for col in train.columns if col not in ['ID', 'Segment']]\n",
    "\n",
    "fixed_columns_B = {col: train_B[col].iloc[0] for col in cols_to_check if is_fixed_column(train_B, col)}\n",
    "fixed_cols = list(fixed_columns_B.keys())\n",
    "print(f\"ğŸ“¦ ê³ ì •ëœ ì¹¼ëŸ¼ {len(fixed_cols)}ê°œ ì œê±°í•  ì˜ˆì •ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "matching_ids_train = train.copy()\n",
    "for col, value in fixed_columns_B.items():\n",
    "    matching_ids_train = matching_ids_train[matching_ids_train[col] == value]\n",
    "matching_ids_train_list = matching_ids_train.groupby('ID').filter(lambda x: len(x) == 6)['ID'].unique()\n",
    "\n",
    "matching_ids_test = test.copy()\n",
    "for col, value in fixed_columns_B.items():\n",
    "    matching_ids_test = matching_ids_test[matching_ids_test[col] == value]\n",
    "matching_ids_test_list = matching_ids_test.groupby('ID').filter(lambda x: len(x) == 6)['ID'].unique()\n",
    "\n",
    "train_filtered = train[train['ID'].isin(matching_ids_train_list)].drop(columns=fixed_cols)\n",
    "test_filtered = test[test['ID'].isin(matching_ids_test_list)].drop(columns=fixed_cols)\n",
    "\n",
    "print(f\"ğŸš€ ìµœì¢… train ë°ì´í„° shape: {train_filtered.shape}\")\n",
    "print(f\"ğŸš€ ìµœì¢… test ë°ì´í„° shape: {test_filtered.shape}\")\n",
    "train_filtered.to_csv('./train/train_vips_B.csv', index=False)\n",
    "test_filtered.to_csv('./test/test_vips_B.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780e4a0d-33ec-4822-8a1f-e93c7238af98",
   "metadata": {},
   "source": [
    "## Modeling - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ca804-63be-45ee-a176-dc3622338b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train/train_vips_A.csv')\n",
    "test = pd.read_csv('./test/test_vips_A.csv')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train['Segment'] = label_encoder.fit_transform(train['Segment'])\n",
    "\n",
    "X = train.drop(columns=['Segment', 'ID'])\n",
    "y = train['Segment']\n",
    "X_test = test.drop(columns=['ID'])\n",
    "\n",
    "cat_features = [col for col in X.columns if X[col].dtype == 'object']\n",
    "for col in cat_features:\n",
    "    X[col] = X[col].astype(str)\n",
    "    X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "params = {\n",
    "    'iterations': 2000,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'MultiClass',\n",
    "    'verbose': 100,\n",
    "    'random_seed': 42,\n",
    "    'task_type': 'GPU',\n",
    "    'class_weights': [20, 50, 2, 1, 1],\n",
    "}\n",
    "\n",
    "n_classes = 5\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸš€ ë‹¨ì¼ Model Run ì‹œì‘\")\n",
    "all_test_probs = np.zeros((X_test.shape[0], n_classes))\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"ğŸ“‚ Fold {fold + 1}\")\n",
    "    X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=(X_valid_fold, y_valid_fold),\n",
    "        cat_features=cat_features,\n",
    "        early_stopping_rounds=100,\n",
    "        use_best_model=True\n",
    "    )\n",
    "    test_probs = model.predict_proba(X_test)\n",
    "    all_test_probs += test_probs\n",
    "\n",
    "avg_test_probs = all_test_probs / kf.get_n_splits()\n",
    "prob_df = pd.DataFrame(avg_test_probs, columns=[0, 1, 2, 3, 4])\n",
    "prob_df['ID'] = test['ID'].values\n",
    "\n",
    "mean_probs = prob_df.groupby('ID').mean().reset_index()\n",
    "mean_probs['Segment'] = mean_probs[[0, 1, 2, 3, 4]].idxmax(axis=1)\n",
    "segment_mapping = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E'}\n",
    "mean_probs['Segment'] = mean_probs['Segment'].map(segment_mapping)\n",
    "\n",
    "a_ids = mean_probs.loc[mean_probs['Segment'] == 'A', 'ID'].tolist()\n",
    "print(f\"\\nâœ… Aë¡œ ë¶„ë¥˜ëœ ID ìˆ˜ = {len(a_ids)}ê°œ\")\n",
    "print(f\"ğŸ” A ID: {a_ids[:50]}\")\n",
    "train = pd.read_csv('./train/train_vips_B.csv')\n",
    "test = pd.read_csv('./test/test_vips_B.csv')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train['Segment'] = label_encoder.fit_transform(train['Segment'])\n",
    "\n",
    "X = train.drop(columns=['Segment', 'ID'])\n",
    "y = train['Segment']\n",
    "X_test = test.drop(columns=['ID'])\n",
    "\n",
    "cat_features = [col for col in X.columns if X[col].dtype == 'object']\n",
    "for col in cat_features:\n",
    "    X[col] = X[col].astype(str)\n",
    "    X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "params = {\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.03,\n",
    "    'depth': 8,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'MultiClass',\n",
    "    'verbose': 100,\n",
    "    'random_seed': 42,\n",
    "    'task_type': 'GPU',\n",
    "    'class_weights': [10, 10, 1, 1, 1],\n",
    "}\n",
    "\n",
    "n_classes = 5\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸš€ ë‹¨ì¼ Model Run ì‹œì‘\")\n",
    "all_test_probs = np.zeros((X_test.shape[0], n_classes))\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"ğŸ“‚ Fold {fold + 1}\")\n",
    "    X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=(X_valid_fold, y_valid_fold),\n",
    "        cat_features=cat_features,\n",
    "        early_stopping_rounds=100,\n",
    "        use_best_model=True\n",
    "    )\n",
    "    test_probs = model.predict_proba(X_test)\n",
    "    all_test_probs += test_probs\n",
    "\n",
    "avg_test_probs = all_test_probs / kf.get_n_splits()\n",
    "prob_df = pd.DataFrame(avg_test_probs, columns=[0, 1, 2, 3, 4])\n",
    "prob_df['ID'] = test['ID'].values\n",
    "\n",
    "mean_probs = prob_df.groupby('ID').mean().reset_index()\n",
    "mean_probs['Segment'] = mean_probs[[0, 1, 2, 3, 4]].idxmax(axis=1)\n",
    "segment_mapping = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E'}\n",
    "mean_probs['Segment'] = mean_probs['Segment'].map(segment_mapping)\n",
    "\n",
    "b_ids = mean_probs.loc[mean_probs['Segment'] == 'B', 'ID'].tolist()\n",
    "print(f\"\\nâœ… Bë¡œ ë¶„ë¥˜ëœ ID ìˆ˜ = {len(b_ids)}ê°œ\")\n",
    "print(f\"ğŸ” B ID: {b_ids[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb4944d-68fc-4653-9fc3-5585c5207584",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e386e-4357-47fc-a138-aeb59630e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = pd.read_csv('./test/base_catboost_kfold.csv')\n",
    "base_df.loc[base_df['ID'].isin(a_ids), 'Segment'] = 'A'\n",
    "base_df.loc[base_df['ID'].isin(b_ids), 'Segment'] = 'B'\n",
    "base_df.to_csv('./test/final_catboost.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Segmentê°€ 'A'ë¡œ ìˆ˜ì •ëœ {len(a_ids)}ê°œ ID ë°˜ì˜ ì™„ë£Œ\")\n",
    "print(f\"âœ… Segmentê°€ 'B'ë¡œ ìˆ˜ì •ëœ {len(b_ids)}ê°œ ID ë°˜ì˜ ì™„ë£Œ\")\n",
    "print(\"ğŸ¯ ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: final_catboost.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python te",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
